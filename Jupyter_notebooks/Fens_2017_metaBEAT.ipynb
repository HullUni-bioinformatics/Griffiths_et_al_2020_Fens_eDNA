{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pumped Catchment eDNA metabarcoding data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use [metaBEAT](https://github.com/HullUni-bioinformatics/metaBEAT), a tool tailored towards reproducible and efficient analyses of metabarcoding data that was developed by Dr. Christoph Hahn (University of Graz) for the EvoHull group at University of Hull. The pipeline is still under active development and will likely be extended further in the future. The pipeline is available in a Docker container with all necessary dependencies. The Docker image builds on [ReproPhylo](https://github.com/HullUni-bioinformatics/ReproPhylo).\n",
    "\n",
    "The metaBEAT tool is designed for complete bioinformatic analysis from raw data, and performs (optionally) de-multiplexing, quality filtering, chimera detection, clustering, and taxononomic assignment (outputs in `.biom` and `.tsv` formats). It currently supports BLAST, Kraken and phylogenetic placement (pplacer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will perform basic processing (trimming, merging, chimera removal, clustering and taxonomic assignment) of the metabarcoding data.\n",
    "\n",
    "Minimum input for an analysis is a set of query sequences in one or several files (a number of file formats are accepted, e.g. `.fasta`, `.fastq`). These will be run through the pipeline sequentially.\n",
    "\n",
    "Information on the nature and location of the query sequence files must be provided in a separate tab-delimited text file via the -Q flags.\n",
    "\n",
    "Each line in this text file should look as follows: unique sample_ID, format, file1, file2\n",
    "\n",
    "The required text files can be generated in any text editor. So theoretically, nano could be used in the terminal to construct the text file. For reproducibility and ease, a simple program can be used to generate the required file.\n",
    "\n",
    "In the cell below, it is produced using a simple python script. The script will list all files in the location to which you downloaded your Illumina data (specified via the 'datadir' variable). It assumes that there is a file ending in `_R1.fastq` for each sample. For each such file, it will extract the sample name from the filename and format the required line for the text file accordingly. The resulting file is called `Querymap.txt` (specified in the 'to' variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 1-trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 1-trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -1 ../raw_reads/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a text file specifying the samples to be processed, including the format and location of the reads - the querymap.\n",
    "\n",
    "The next command expects two `.fastq` files (forward and reverse reads) per sample in the directory `../raw_reads/`. It expects the files to be named 'SampleID', followed by '_R1' or '_R2' to identify the forward/reverse read file respectively.\n",
    "\n",
    "The raw data have been downloaded and demultiplexed. They can be found in `../raw_reads/`.\n",
    "\n",
    "SampleID must correspond to the first column in a file called `Sample_accessions.tsv`. This will either be pre-made to correspond to downloading read data from the NCBI Sequence Read Archive, or you will have to make it. The marker is '12S'.\n",
    "\n",
    "If the `Sample_accessions.tsv` was pre-made, use this code to proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#\n",
    "#for a in $(cat ../Data/Sample_accessions.tsv | grep \"12S\" | cut -f 1 | grep \"SampleID\" -v)\n",
    "#do\n",
    "#    R1=$(ls -1 ../raw_reads/$a-12S_* | grep \"_R1.fastq\")\n",
    "#    R2=$(ls -1 ../raw_reads/$a-12S_* | grep \"_R2.fastq\")\n",
    "#\n",
    "#    echo -e \"$a\\tfastq\\t$R1\\t$R2\"\n",
    "#done > Querymap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 Querymap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR...**\n",
    "\n",
    "To make the `Sample_accessions.tsv` file, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"SampleID\" > ../1-trimming/Sample_accessions.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for a in $(ls ../raw_reads/ | grep -w \"R1\" | cut -d '.' -f 1)\n",
    "do \n",
    "   SampleID=$a\n",
    "   \n",
    "   echo -e \"$SampleID\"\n",
    "done >> ../1-trimming/Sample_accessions.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../1-trimming/Sample_accessions.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for a in $(cat ../1-trimming/Sample_accessions.tsv | grep \"SampleID\" -v)\n",
    "do\n",
    "    R1=$(ls -1 ../raw_reads/$a.* | grep -w \"R1\")\n",
    "    R2=$(ls -1  ../raw_reads/$a.* | grep -w \"R2\")\n",
    "\n",
    "    echo -e \"$a\\tfastq\\t$R1\\t$R2\"\n",
    "done > Querymap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 Querymap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the `Querymap.txt` file, add two columns which specify the number of bases to remove from the forward and reverse read. In our case, we want to remove 18 bp to ensure that there is no forward or reverse primer left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sed 's/$/&\\t18/' Querymap.txt > Querymap_new.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look (note that the output is probably line-wrapped):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 4 Querymap_new.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sed 's/$/&\\t18/' Querymap_new.txt > Querymap_final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 4 Querymap_final.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw read processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform basic quality trimming and clipping (Trimmomatic) and paired-end read merging (flash). metaBEAT will be used to process all samples in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!metaBEAT_global.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to trim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo -e \"Starttime: $(date)\\n\"\n",
    "\n",
    "metaBEAT_global.py \\\n",
    "-Q Querymap_final.txt \\\n",
    "--trim_qual 30 \\\n",
    "--read_crop 110 \\\n",
    "--trim_minlength 90 \\\n",
    "--merge \\\n",
    "--product_length 106 \\\n",
    "--forward_only \\\n",
    "--length_filter 106 \\\n",
    "--length_deviation 0.2 \\\n",
    "-m 12S -o Eel2017_trim30-min90-crop110-forwonly-filter100-deviation0.2 \\\n",
    "-n 5 -v \\\n",
    "-@ youremail@server.com &> log\n",
    "\n",
    "echo -e \"Endtime: $(date)\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read processing will take several hours.\n",
    "\n",
    "\n",
    "# Visualise query survival after trimming\n",
    "\n",
    "metaBEAT will generate a directory with all temporary files that were created during the processing for each sample and will record useful stats summarizing the data processing in the file `metaBEAT_read_stats.csv`. You can explore the table manually or quickly plot out some of these stats here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Eel2017_trim30-min90-crop110-forwonly-filter100-deviation0.2_read_stats.csv', index_col=0)\n",
    "df['fraction'] = df['queries']/(df['total']*0.5)\n",
    "df.fraction.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed information on what metaBEAT did to each sample is contained in the `log` file. It contains the exact commands that were run for each sample during each step of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 100 log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps in the processing will be chimera detection, and global clustering of the centroids from all clusters from all samples to produce denovo OTUs. The temporary files from the global clustering and the final OTU table were written to the directory `./GLOBAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls GLOBAL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denovo OTU table (numbers are reads) can be viewed to see how OTUs are distributed across your samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chimera detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stats on the read counts before/after trimming, merging etc. are summarised for you in `metaBEAT_read_stats.csv`.\n",
    "\n",
    "Next stage of the processing is chimera detection and removal of putative chimeric sequences. We'll do that using uchime as implemented in vsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 2-chimera_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 2-chimera_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert reference database from GenBank to fasta format to be used in chimera detection.\n",
    "\n",
    "Prepare `REFmap.txt` file, i.e. text file that specifies the location and the format of the reference to be used.\n",
    "The reference sequences in GenBank format are present in subdirectories for each vertebrate group in the `../Reference_database` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo '../supplementary_data/Reference_DBs/12S_Fish_SATIVA_cleaned_May_2017.gb\\tgb\\n' \\\n",
    "'../supplementary_data/Reference_DBs/M.zebra.gb\\tgb'> REFmap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat REFmap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!metaBEAT_global.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "metaBEAT_global.py \\\n",
    "-R REFmap.txt \\\n",
    "-f \\\n",
    "-@ youremail@server.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce `refs.fasta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head refs.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run chimera detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "for a in $(cut -f 1 ../1-trimming/Querymap.txt)\n",
    "do\n",
    "    if [ -s ../1-trimming/$a/$a\\_trimmed.fasta ]\n",
    "    then\n",
    "        echo -e \"\\n### Detecting chimeras in $a ###\\n\"\n",
    "        mkdir $a\n",
    "        cd $a\n",
    "        vsearch --uchime_ref ../../1-trimming/$a/$a\\_trimmed.fasta --db ../refs.fasta \\\n",
    "        --nonchimeras $a-nonchimeras.fasta --chimeras $a-chimeras.fasta &> log \n",
    "        cd ..\n",
    "\n",
    "    else\n",
    "        echo -e \"$a is empty\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and taxonomic assignment against UK fish database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 3-taxonomic_assignment_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 3-taxonomic_assignment_fish/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the text file containing the fish reference sequences using the command line - we call it `REFmap.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo '../supplementary_data/Reference_DBs/12S_Fish_SATIVA_cleaned_May_2017.gb\\tgb\\n' \\\n",
    "'../supplementary_data/Reference_DBs/M.zebra.gb\\tgb'> REFmap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat REFmap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the text file containing non-chimera query sequences - `Querymap.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#Querymap\n",
    "for a in $(ls -l ../2-chimera_detection/ | grep \"^d\" | perl -ne 'chomp; @a=split(\" \"); print \"$a[-1]\\n\"')\n",
    "do\n",
    "    echo -e \"$a-nc\\tfasta\\t../2-chimera_detection/$a/$a-nonchimeras.fasta\"\n",
    "done > Querymap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Querymap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Querymap.txt file has been made but includes the `./GLOBAL` directory in which all centroids and queries are contained. This will cause metaBEAT to fail so it must be removed manually from the `Querymap.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed '/GLOBAL/d' Querymap.txt > Querymap_final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Querymap_final.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the taxonomy database in the current metaBEAT image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!taxit new_database \\\n",
    "--taxdump-url ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_archive/taxdmp_2018-10-01.zip \\\n",
    "    --download-dir /usr/bin/ /usr/bin/taxonomy.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost it. Now start the pipeline to do sequence clustering and taxonomic assignment of non-chimera queries via metaBEAT. As input, `Querymap.txt` containing samples that have been trimmed, merged and checked for chimeras, and the `REFmap.txt` file must be specified. metaBEAT will be asked to attempt taxonomic assignment using BLAST.\n",
    "\n",
    "metaBEAT will automatically wrangle the data into the particular file formats that are required by each of the methods, run all necessary steps, and finally convert the outputs of each program to a standardized BIOM table.\n",
    "\n",
    "GO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!metaBEAT_global.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo -e \"Starttime: $(date)\\n\"\n",
    "\n",
    "metaBEAT_global.py \\\n",
    "-Q Querymap_final.txt \\\n",
    "-R REFmap.txt \\\n",
    "--cluster --clust_match 1 --clust_cov 3 \\\n",
    "--blast --min_ident 0.98 \\\n",
    "-m 12S -n 5 \\\n",
    "-E -v \\\n",
    "-@ youremail@server.com \\\n",
    "-o Eel2017_12S-trim30-min90-crop110-mergeforwonly-filt100-dev0.2_nonchimera_c1cov3_blast0.98_fish &> log_fish\n",
    "\n",
    "echo -e \"Endtime: $(date)\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 50 log_fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DONE, Output file under GLOBAL/BLAST_0.98"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
